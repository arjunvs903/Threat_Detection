{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395ea3f9-4ed9-41a0-8a94-44534cec3bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-05 01:40:59] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /home/arjun/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/miniconda3/envs/pytorch/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "[2023-08-05 01:41:01] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2023-08-05 01:41:01] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2023-08-05 01:41:01] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2023-08-05 01:41:01] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
      "[2023-08-05 01:41:06] INFO - checkpoint_utils.py - Successfully loaded model weights from checkpoints/wd_yolonas_run/ckpt_best.pth EMA checkpoint.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from super_gradients.training import models\n",
    "best_model = models.get('yolo_nas_l',\n",
    "                        num_classes=3,\n",
    "                        checkpoint_path=\"checkpoints/wd_yolonas_run/ckpt_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c28091-60a3-47ed-963b-d8d85d248218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-05 02:43:37] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input_video_path = \"test/images/k001.jpg\"\n",
    "output_video_path = \"test/out/k001.jpg\"\n",
    "#device=0\n",
    "\n",
    "res = best_model.predict(input_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7c06183-9910-4cfa-96f1-9df65af9d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageDetectionPrediction(image=array([[[180, 178, 163],\n",
      "        [180, 178, 163],\n",
      "        [181, 179, 164],\n",
      "        ...,\n",
      "        [186, 175, 157],\n",
      "        [185, 174, 156],\n",
      "        [185, 174, 156]],\n",
      "\n",
      "       [[181, 179, 164],\n",
      "        [181, 179, 164],\n",
      "        [181, 179, 164],\n",
      "        ...,\n",
      "        [184, 176, 157],\n",
      "        [184, 176, 157],\n",
      "        [183, 175, 156]],\n",
      "\n",
      "       [[181, 179, 164],\n",
      "        [181, 179, 164],\n",
      "        [181, 179, 164],\n",
      "        ...,\n",
      "        [185, 177, 158],\n",
      "        [184, 176, 157],\n",
      "        [184, 176, 157]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[123,  97,  62],\n",
      "        [129, 103,  68],\n",
      "        [127, 101,  68],\n",
      "        ...,\n",
      "        [223, 222, 217],\n",
      "        [222, 221, 216],\n",
      "        [222, 221, 216]],\n",
      "\n",
      "       [[126, 100,  65],\n",
      "        [123,  97,  62],\n",
      "        [120,  94,  61],\n",
      "        ...,\n",
      "        [223, 222, 217],\n",
      "        [223, 222, 217],\n",
      "        [223, 222, 217]],\n",
      "\n",
      "       [[125,  99,  64],\n",
      "        [115,  89,  54],\n",
      "        [116,  90,  57],\n",
      "        ...,\n",
      "        [223, 224, 216],\n",
      "        [223, 224, 216],\n",
      "        [223, 224, 216]]], dtype=uint8), prediction=DetectionPrediction(bboxes_xyxy=array([[ 122.81365, 2433.9492 , 3860.9626 , 8257.033  ],\n",
      "       [2071.5112 , 6568.0186 , 3743.288  , 7322.388  ]], dtype=float32), confidence=array([0.9347859, 0.6160722], dtype=float32), labels=array([0., 1.], dtype=float32)), class_names=['Person', 'Knife', 'Gun'])\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "168441d7-ac72-4915-9bd5-1acea697328c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 178, 163],\n",
       "        [180, 178, 163],\n",
       "        [181, 179, 164],\n",
       "        ...,\n",
       "        [186, 175, 157],\n",
       "        [185, 174, 156],\n",
       "        [185, 174, 156]],\n",
       "\n",
       "       [[181, 179, 164],\n",
       "        [181, 179, 164],\n",
       "        [181, 179, 164],\n",
       "        ...,\n",
       "        [184, 176, 157],\n",
       "        [184, 176, 157],\n",
       "        [183, 175, 156]],\n",
       "\n",
       "       [[181, 179, 164],\n",
       "        [181, 179, 164],\n",
       "        [181, 179, 164],\n",
       "        ...,\n",
       "        [185, 177, 158],\n",
       "        [184, 176, 157],\n",
       "        [184, 176, 157]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[123,  97,  62],\n",
       "        [129, 103,  68],\n",
       "        [127, 101,  68],\n",
       "        ...,\n",
       "        [223, 222, 217],\n",
       "        [222, 221, 216],\n",
       "        [222, 221, 216]],\n",
       "\n",
       "       [[126, 100,  65],\n",
       "        [123,  97,  62],\n",
       "        [120,  94,  61],\n",
       "        ...,\n",
       "        [223, 222, 217],\n",
       "        [223, 222, 217],\n",
       "        [223, 222, 217]],\n",
       "\n",
       "       [[125,  99,  64],\n",
       "        [115,  89,  54],\n",
       "        [116,  90,  57],\n",
       "        ...,\n",
       "        [223, 224, 216],\n",
       "        [223, 224, 216],\n",
       "        [223, 224, 216]]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e07a6e-9eee-4efc-a2f3-325bfe21b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-05 00:48:06] INFO - checkpoint_utils.py - Successfully loaded model weights from checkpoints/wd_yolonas_run/ckpt_best.pth EMA checkpoint.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YoloNAS_L' object has no attribute 'detect_video'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Detect objects in the frame.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_video\u001b[49m(frame, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Show the detections on the frame.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m detections:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'YoloNAS_L' object has no attribute 'detect_video'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1144.490] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@1144.490] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from super_gradients.training import models\n",
    "\n",
    "# Load the model.\n",
    "best_model = models.get('yolo_nas_l',\n",
    "                        num_classes=3,\n",
    "                        checkpoint_path=\"checkpoints/wd_yolonas_run/ckpt_best.pth\")\n",
    "\n",
    "# Create a video capture object.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the video.\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect objects in the frame.\n",
    "    detections = best_model.detect_video(frame, imgsz=640, conf=0.5)\n",
    "\n",
    "    # Show the detections on the frame.\n",
    "    for detection in detections:\n",
    "        cv2.rectangle(frame, detection['bounding_box'], (255, 0, 0), 2)\n",
    "        label = f\"{detection['class_id']}: {detection['confidence']:.2f}\"\n",
    "        cv2.putText(frame, label, (detection['bounding_box'][0], detection['bounding_box'][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame.\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Quit if the user presses the 'q' key.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object.\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ab823c-7e6d-4c98-90d9-cfcb011d68f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to access the camera.\n",
      "Error: Unable to receive frames from the camera.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@975.456] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@975.456] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "    # Check if the camera was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "\n",
    "while True:\n",
    "        # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "        # If the frame was not read properly, break the loop\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to receive frames from the camera.\")\n",
    "        break\n",
    "\n",
    "        # Display the captured frame\n",
    "    cv2.imshow('Live Camera', frame)\n",
    "\n",
    "        # Stop capturing when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Release the video capture object and close the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ff194-0e7b-47a8-a28a-c52f67de02a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
